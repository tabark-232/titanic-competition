# -*- coding: utf-8 -*-
"""Titanic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19WpqbbWTVVQEYRwn2ulmVxSV2XA-iQiv
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

df=pd.read_csv("train.csv")

df.head()

df.info()

df.describe()

"""#correlation"""

correlation = df['Survived'].corr(df['Pclass'])

print(f' {correlation}')

correlation = df['Age'].corr(df['Survived'])

print(f' {correlation}')

correlation = df['Fare'].corr(df['Survived'])

print(f' {correlation}')

import seaborn as sns
plt.figure(figsize=(8,6))
sns.boxplot(x='Embarked', y='Survived', data=df)
plt.title('مخطط Box Plot بين Survired و Embarked')
plt.show()

#import seaborn as sns
#plt.figure(figsize=(10,6))
#sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
#plt.show()

print(df.groupby('Embarked')['Survived'].mean())

"""#missing data"""

df['Age'].fillna(df['Age'].mean(),inplace=True)

df['Embarked'].fillna(df['Embarked'].mode()[0],inplace=True)

df.drop('Cabin',axis=1,inplace=True)

df.drop('PassengerId',axis=1,inplace=True)

#df.drop('Name',axis=1,inplace=True)

df.drop('Ticket',axis=1,inplace=True)

#from sklearn.feature_selection import RFE
#from sklearn.ensemble import RandomForestClassifier

#model = RandomForestClassifier(n_estimators=100)
#selector = RFE(model, n_features_to_select=5)
#selector.fit(x_train, y_train)

#selected_features = x.columns[selector.support_]
#print("أفضل الميزات:", selected_features)

"""#encoding"""

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
df['Sex'] = label_encoder.fit_transform(df['Sex'])

df.Sex.value_counts()

df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)

df.Embarked_Q.value_counts()

df.Embarked_S.value_counts()

df.Sex

"""#Feature Engineering"""

df['Title'] = df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
df['Title'] = label_encoder.fit_transform(df['Title'])

df['FamilySize'] = df['SibSp'] + df['Parch'] + 1

"""#splitting"""

x = df[['Age', 'Pclass', 'Sex', 'Fare','FamilySize','Embarked_S','Title']] # Include the one-hot encoded columns for 'Embarked'
y = df['Survived']

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)

"""#feature"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

"""#logisticRegression"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(x_train, y_train)

"""#xgboost"""

from xgboost import XGBClassifier
classifier = XGBClassifier()
classifier.fit(x_train, y_train)

y_pred = classifier.predict(x_test)

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
}

# Create GridSearchCV object
grid_search = GridSearchCV(
    XGBClassifier(random_state=42),
    param_grid=param_grid,  # Pass the parameter grid here
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

# Fit the model
grid_search.fit(x_train, y_train)

# Print the best parameters
print("Best parameters:", grid_search.best_params_)

best_params = grid_search.best_params_

xgb_best = XGBClassifier(**best_params, random_state=42)
xgb_best.fit(x_train, y_train)

# Prediction and Accuracy Calculation
y_pred_best = xgb_best.predict(x_test)
accuracy_best = accuracy_score(y_test, y_pred_best)

print(f'Accuracy after parameter adjustment: {accuracy_best:.4f}')